{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Checking to make sure output directories are created..\n",
      "..done\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import models.simpleConv as simpleConv\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "import h5py\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "import utils\n",
    "import preprocessing as pp\n",
    "pp.init()\n",
    "\n",
    "allTrainingFolders = [x for x in os.listdir(pp.trainPath) if x.startswith('n')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing images for class ID: n01443537\n",
      "Parsing images for class ID: n01491361\n",
      "Parsing images for class ID: n01494475\n",
      "Parsing images for class ID: n01440764\n",
      "Parsing images for class ID: n01484850\n",
      "Train on 2070 samples, validate on 519 samples\n",
      "Epoch 1/1\n",
      "2070/2070 [==============================] - 148s 72ms/step - loss: 1.1588 - acc: 0.4768 - val_loss: 1.1205 - val_acc: 0.5222\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelBinarizer()\n",
    "##MAKE SURE classLabels is set to ALL the folders you will train on, even if doing in batches\n",
    "encoder = encoder.fit(allTrainingFolders) \n",
    "\n",
    "parse = False\n",
    "includeAugmented = False\n",
    "onlyAugmented = False\n",
    "augments = [iaa.GaussianBlur(3.0)]\n",
    "h5filename = 'all.h5'\n",
    "\n",
    "if parse:\n",
    "    pp.parseImages(trainingFolders, h5filename)\n",
    "    \n",
    "#NOTE: KEEP BATCH SIZE = # of all folders for now because we need to shuffle H5. \n",
    "batch_size = 5\n",
    "folderBatches = utils.chunks(allTrainingFolders, batch_size)\n",
    "# print(folderBatches)\n",
    "\n",
    "callbacks = []\n",
    "epochs = 1\n",
    "\n",
    "model = simpleConv.build_model(pp.img_width, pp.img_height, 3)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "all_x_val = []\n",
    "all_y_val = []\n",
    "all_x_train = []\n",
    "all_y_train = []\n",
    "\n",
    "# for batch in folderBatches:\n",
    "    ##LOAD IN DATA\n",
    "\n",
    "x_train, y_train, x_val, y_val = pp.loadH5(h5filename)\n",
    "\n",
    "x_aug = []\n",
    "y_aug = []\n",
    "if includeAugmented:\n",
    "    x_aug, y_aug = pp.augmentData(x_train, y_train, augments = augments)\n",
    "    if onlyAugmented:\n",
    "        x_train = x_aug\n",
    "        y_train = y_aug\n",
    "    else:\n",
    "        x_train.extend(x_aug)\n",
    "        y_train.extend(y_aug)\n",
    "\n",
    "\n",
    "y_train = encoder.transform(y_train) #call encoder.inverse_transform() to get real class labels\n",
    "y_val = encoder.transform(y_val)\n",
    "\n",
    "#this is only needed if you are doing individual batches somehow. ignore for now. \n",
    "all_y_val.extend(y_val)\n",
    "all_x_val.extend(x_val)\n",
    "all_y_train.extend(y_train)\n",
    "all_x_train.extend(x_train)\n",
    "\n",
    "\n",
    "##FIT MODEL\n",
    "\n",
    "# print(\"Fitting model on folder batch: {0}\".format(batch))\n",
    "results = model.fit(np.array(x_train), np.array(y_train), \n",
    "                    validation_data = (np.array(x_val), np.array(y_val)),\n",
    "                    epochs=epochs, verbose=1, callbacks = callbacks)\n",
    "    \n",
    "\n",
    "#     model.train_on_batch(np.array(x_train), np.array(y_train))\n",
    "\n",
    "    \n",
    "#TODO:\n",
    "#1. Need to shuffle the data that we're loading in, so that each call to fit has a sample for each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519/519 [==============================] - 10s 19ms/step\n",
      "2070/2070 [==============================] - 42s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "val_scores = model.evaluate(np.array(all_x_val), np.array(all_y_val))\n",
    "train_scores = model.evaluate(np.array(all_x_train), np.array(all_y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "val: [0.9152405714483389, 0.5761078999795896]\n",
      "train: [0.8406962091796064, 0.6183574880954724]\n"
     ]
    }
   ],
   "source": [
    "print(model.metrics_names)\n",
    "print(\"val: {0}\".format(val_scores))\n",
    "print(\"train: {0}\".format(train_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL: BUILDING + TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4140 samples, validate on 519 samples\n",
      "Epoch 1/1\n",
      "4140/4140 [==============================] - 271s 65ms/step - loss: 1.0210 - acc: 0.5471 - val_loss: 0.9161 - val_acc: 0.6050\n"
     ]
    }
   ],
   "source": [
    "callbacks = []\n",
    "epochs = 1\n",
    "\n",
    "model = simpleConv.build_model(pp.img_width, pp.img_height, 3)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "results = model.fit(np.array(x_train), np.array(y_train), \n",
    "                    validation_data = (np.array(x_val), np.array(y_val)),\n",
    "                    epochs=epochs, verbose=1, callbacks = callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(np.array(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias', 0.41387945), ('tiger shark, Galeocerdo cuvieri', 0.2816862), ('hammerhead, hammerhead shark', 0.19379242)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#PRINT PREDICTION CLASSES\n",
    "print(pp.topClasses(predictions[504], encoder.classes_))\n",
    "\n",
    "#LOAD PREDICTIONS INTO DATAFRAME, compute accuracy\n",
    "def predictionsToDataframe(predictions, truth, encoder):\n",
    "    one = []\n",
    "    two = []\n",
    "    three = []\n",
    "    for p in predictions:\n",
    "        top = pp.topClasses(p, encoder.classes_)\n",
    "        one.append(top[0][0])\n",
    "        two.append(top[1][0])\n",
    "        three.append(top[2][0])\n",
    "        \n",
    "    df = pd.DataFrame({'truth': [pp.translateID(x) for x in encoder.inverse_transform(truth)],\n",
    "                      'one': one,\n",
    "                      'two': two,\n",
    "                      'three': three}) \n",
    "\n",
    "    return df\n",
    "\n",
    "df = predictionsToDataframe(predictions, y_val, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.5221579961464354\n",
      "Top-2 accuracy: 0.7398843930635838\n",
      "Top-3 accuracy: 0.9248554913294798\n"
     ]
    }
   ],
   "source": [
    "acc = len(df[df.truth == df.one])/len(df)\n",
    "print(\"Top-1 accuracy: {0}\".format(acc))\n",
    "\n",
    "acc = len(df[(df.truth == df.one) | (df.truth == df.two)])/len(df)\n",
    "print(\"Top-2 accuracy: {0}\".format(acc))\n",
    "\n",
    "acc = len(df[(df.truth == df.one) | (df.truth == df.two) | (df.truth == df.three) ]) / len(df)\n",
    "print(\"Top-3 accuracy: {0}\".format(acc))\n",
    "\n",
    "# df.iloc[504]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "1. Parse Image Data for all folders that you want to train on\n",
    "2. Load in each batch and repeat:\n",
    "    3. Apply augments to batch, if any\n",
    "    4. Train batch\n",
    "5. Evaluate model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n01443537', 'n01491361']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Wish list: \n",
    "\n",
    "# top-k accuracy during training\n",
    "# training on batches in a smart way: batch generator and database on disk, probably. \n",
    "# look into nb_epoch and data generators\n",
    "# https://stackoverflow.com/questions/33900486/shuffle-hdf5-dataset-using-h5py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
